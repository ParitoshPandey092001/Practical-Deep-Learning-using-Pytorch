# -*- coding: utf-8 -*-
"""Tensors

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mCLoByy54lvGQ64fj3Y01ELxPke04VgI
"""

import torch
print(torch.__version__)

if torch.cuda.is_available():
    print("GPU is available!")
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    print("GPU not available. Using CPU.")

"""Creating Tensor"""

# using empty
a = torch.empty(2,3)
print(a)

# check type
type(a)

# using zeros
torch.zeros(2,3)

# using ones
torch.ones(2,3)

# using rand
torch.rand(2,3)   # useful in creating weight initialization matrix

# use of seed
torch.rand(2,3)  # different value as above

# manual_seed
torch.manual_seed(100)
torch.rand(2,3)

# manual_seed
torch.manual_seed(100)
torch.rand(2,3)              # getting same values

# using tensor

torch.tensor([[1,2,3],[4,5,6]])  # make custom tensors

# other ways

# arange
print("using arange ->", torch.arange(0,10,2))

# using linspace
print("using linspace ->", torch.linspace(0,10,10))

# using eye
print("using eye ->", torch.eye(5))  # 5X5 identity matrix

# using full
print("using full ->", torch.full((3, 3), 5))

"""Tensor Shapes"""

X = torch.tensor([[1,2,3],[4,5,6]])
print(X.shape)

torch.empty_like(X)

torch.zeros_like(X)

torch.ones_like(X)

torch.rand_like(X, dtype = torch.float)

"""Mathematical Operations

1) Scalar Operations
"""

x = torch.rand(2,2)
x

# addition
x + 2
# substraction
x - 2
# multiplication
x * 3
# division
x / 3
# int division
(x * 100)//3
# mod
((x * 100)//3)%2
# power
x**2

"""2) Element wise operation"""

a = torch.rand(2,3)
b = torch.rand(2,3)

print(a)
print(b)

# add
a + b
# sub
a - b
# multiply
a * b
# division
a / b
# power
a ** b
# mod
a % b

c = torch.tensor([1, -2, 3, -4])

#abs
torch.abs(c)

d = torch.tensor([1.9, 2.3, 3.7, 4.4])

# round
torch.round(d)

# ceil
torch.ceil(d)

#floor
torch.floor(d)

# clamp
torch.clamp(d, min = 2, max = 3)

"""3) Reduction operation"""

e = torch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)
e

# sum
torch.sum(e)

# sum along columns
torch.sum(e, dim=0)

# sum along rows
torch.sum(e, dim=1)

# mean
torch.mean(e) # net total mean
# mean along col
torch.mean(e, dim=0)

#median
torch.median(e)

# max and min
torch.max(e)
torch.min(e)

# argmax
torch.argmax(e)

torch.argmin(e)

"""4) Matrix operations"""

f = torch.randint(size=(2,3), low=0, high=10)
g = torch.randint(size=(3,2), low=0, high=10)

print(f)
print(g)

# matrix multiplication
torch.matmul(f, g)

vector1 = torch.tensor([1, 2])
vector2 = torch.tensor([3, 4])

# dot product
torch.dot(vector1, vector2)

# transpose
torch.transpose(f, 0, 1) #swap dimension 0 and 1

h = torch.randint(size=(3,3), low=0, high=10, dtype=torch.float32)
h

# determinant
torch.det(h)

# inverse
torch.inverse(h)

"""5) Comparison Operations"""

k = torch.randint(size=(2,3), low=0, high=10, dtype=torch.float32)
k

# log
torch.log(k)

# exp
torch.exp(k)

# sqrt
torch.sqrt(k)

# sigmoid
torch.sigmoid(k)

# softmax
torch.softmax(k, dim=0) # across comumns

# relu
torch.relu(k)

"""Inplace Operations"""

m = torch.rand(2,3)
n = torch.rand(2,3)

print(m)
print(n)

m.add_(n)

m

n

torch.relu(m)

m.relu_() # for permanent changes

"""Copying a tensor"""

a = torch.rand(2,3)
a

b = a

b

b = a.clone()

a

b

a[0][0] = 10

a

b # due to clone value of b did not change when we changed a

id(a)

id(b)

